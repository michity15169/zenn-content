---
title: "Spannerの運用について"
emoji: "🔧"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ["cloudspanner", "gcp"]
publication_name: "google_cloud_jp"
published: true
---
## はじめに

Spannerはマネージドデータベースとして提供されているサービスです。そのためパッチ適用やハードウェアのメンテナンスなど、データベースで日常的に発生する運用面での手間の多くは自動的に提供されています。一方で、データベースの運用という観点ではそれ以外の必要な作業や考慮するべき事項、監視ポイントなどがあります。一般的なリレーショナルデータベース（以下、RDB）と同様の操作もありますが、Spanner特有の概念も存在します。これらについて、RDBとの差分を中心に解説します。理由が明らかな内容についてはその背景となる理由も極力記載しました。

## ストレージ

### ストレージサイズ

Spannerではストレージサイズはテーブルに書き込んだデータの量に応じて自動的に拡張されます。そのため、一般的な意味でストレージの残り容量を監視する必要はありません。ただし、コンピュートの1ノード数あたり4TiBというストレージの上限はあるため、これに到達していないかは注意が必要です。
実際の運用上ではノードあたりのストレージサイズの上限に届く前に、コンピュートの性能の上限を迎える事が多いためノード数を増やすことになり、ノードあたりの上限に届くことは少ないでしょう。実際に上限を迎えた場合はノード数を増やすかデータ削除することが対応方法となります。

ストレージの使用量はCloud Monitoringのメトリクスとして確認や監視が可能です。前述の通りストレージの使用量は実際のデータ保持量におうじているためテーブルからデータを削除するとストレージの使用量も減少しますが、実際に使用量が減少するまでにはある程度タイムラグがあります。これはテーブルからの削除操作（DELETE文など）を行ったあとにバックグランド処理としてコンパクション（Compaction）が実行されることでストレージの使用量に反映されるためです。削除直後にストレージの使用量が縮まないことは期待される動作であるため、しばらく時間をおいてメトリクスを確認するとよいです。

### バックアップ

Spannerのストレージとして [Colossus](https://cloud.google.com/blog/ja/products/storage-data-transfer/a-peek-behind-colossus-googles-file-system) という分散ストレージシステムを利用しているため、ストレージ自体がコンポーネント単位の物理障害に対して高い耐久性があります。一方で、リージョン単位の障害やアプリケーションの誤動作や操作ミスによる論理障害などに備えるためのバックアップも可能です。

バックアップはバックアップ元のインスタンスと同じリージョンに取られ、新しいSpannerインスタンスへ復元できます。バックアップはインスタンスのサーバーリソースを使わずにストレージレイヤーから取得されるため、取得中も更新やクエリーのパフォーマンスへの影響はありません。RDBでの物理バックアップに近い考え方です。

注意点として、バックアップは[最大1年の有効期間](https://cloud.google.com/spanner/docs/backup?hl=ja#recommended)があるという点です。これを超えて保存したい場合には次の項目のインポート・エクスポートを使うと良いでしょう。

### インポート・エクスポート

### ポイントインタイムリカバリ(Point in Time Recovery)

[PITR](https://cloud.google.com/spanner/docs/pitr?hl=ja) とはデータベースの内容を特定の時刻に戻すという機能です。アプリケーションの誤動作やオペレーションミスによるデータ破損など、論理障害からの回復手段として利用されます。

一般的なRDBMSではバックアップとトランザクションログ（binlogやWAL、アーカイブログ）と組み合わせて、稼働中のデータベースとは別にリストア先のインスタンスを作られることが多いでしょう。これに対してCloud SpannerのPITRの考え方は少し違います。
Spannerでは、過去特定の時点でのデータをクエリーを行う事ができる機能（ステイル読み取り）を使って過去の時刻を指定した読み取りを行いその内容をテーブルに書き戻すことで特定の時刻の内容を復元します。そのため、特定のテーブルやレコードのみ復元するといったことも可能です。テーブル全体を復元する必要はないけど、特定のレコードの当時の状態を知りたい場合にもステイル読み取りは有効です。

ステイル読み取りで参照が可能な過去の時刻は、デフォルトでは現在時刻よりも1時間前までです。設定により最大7日間まで延長することが可能です。過去のデータの参照可能期間を延長すると更新量に応じてストレージの使用量が増加します。

### 断片化(fragment)対策

多くのRDBMSではテーブルやインデックの作成後にレコードに対して追記・削除・更新を繰り返すとデータの物理レイアウトが断片化を起こしパフォーマンス低下を招きます。この対策として、定期的なインデックスを再構築してメンテナンスを行う必要があります。

Spannerではこのような明示的な操作は不要です。Spannerでは[LSM tree](https://en.wikipedia.org/wiki/Log-structured_merge-tree) と呼ばれるデータ構造を使っています。このデータ構造では追記はもちろん、削除・更新もその場での書き換えではなく追記として実施されます。そのためデータファイルが歯抜けになり断片化するという動作が原理的に発生しません。

断片化に近い事象としては、大量のデータを一気にロードしたり削除したときが考えられます。さきほどの説明の通りこれらの操作は追記によって実現されます。もちろん削除操作を追記しただけではデータの占有サイズが大きくなる一方であるため、バックグラウンドタスクとしてコンパクションが実施されます。これによりデータが再編成されます。コンパクションの操作はシステムタスクとして実装されており、[中間の優先度で実行されています](https://cloud.google.com/spanner/docs/cpu-utilization?hl=ja#task-priority)。そのため、大量のデータ操作をしてからある程度の時間をおいてシステムタスクがCPU時間を使用する様子が観測されることがあります。ある意味定常的に詰め直しが発生しているとも言えます。この仕組みによってデータベース管理の一環として管理者による定期的なインデックスの再編成は必要ありません。

## クエリーオプティマイザー

Spannerには一般的なRDBMSと同様にクエリーオプティマイザーが存在し、オプティマイザーが実行計画を策定してそれに基づき実行されます。2024年9月時点での最新のオプティマイザーはバージョン7で、デフォルトではバージョン6が選択されています。各バージョンのオプティマイザーの詳細は[変更履歴](https://cloud.google.com/spanner/docs/query-optimizer/versions?hl=en#version-history)にありますが、最近のバージョンはコストベースオプティマイザーとなっています。

コストベースオプティマイザーは現在多くのRDBMSで広く採用されている技術です。テーブルやインデックスへのアクセス処理がどの程度のコスト（CPUやメモリ、ストレージなど）がかかるかを積み上げて実行計画の効率を評価するものです。それ以前に使われていたルールベースより実際のテーブルの状況に即した最適な実行計画が得られる事が多いですが、統計情報が変化すると実行計画が変わることが知られています。

オプティマイザーはときどき新バージョンがリリースされており、今後も更新されていく可能性があります。新バージョンはリリース後しばらくはデフォルトでは選択されず、明示的に選択することで利用できます。新バージョンは全体的なパフォーマンス向上を目指していますが、特定のクエリーのパフォーマンスが低下する場合もあります。そのような挙動の変更を避けたい場合には、オプティマイザーのバージョンを固定することも可能です。指定の上書き関係は以下の通りです。

- 優先度：低
- Spannerのデフォルト
- データベースのオプション（ALTER DATABASEで指定）
- クライアントアプリ
- 環境変数
- ステートメントヒント
- 優先度：高

データベースのオプションで指定しておくと永続性もあるため、指定したい場合はこの方法で指定されるのが良いでしょう。その上で、特定の処理で上書きしたいときに、アプリケーションから指定する、あるいは環境変数で指定し動作を確認するなどが良いのではないでしょうか。

## 実行計画

実行計画が意図しないインデックスを使用している場合に、特定のインデックスの使用を強制したい場合があります。そのような場合には[ヒント句](https://cloud.google.com/spanner/docs/reference/standard-sql/query-syntax#table_hints)を使います。ヒント句にはテーブルへのアクセス方法を指定するヒント、JOINのアルゴリズムを指定するヒントなどがあります。

すべてのクエリーでヒント句を個別に付けていくのは現実的ではないので、とくに遅いクエリーや効率の良くないクエリーへピンポイントで対応するのが現実的でしょう。

## 統計情報

Spannerの[オプティマイザーが使う統計情報](https://cloud.google.com/spanner/docs/query-optimizer/manage-query-optimizer?hl=ja#list-statistics-packages)は自動的に作成され、更新されていきます。基本的には自動更新に任せて大丈夫ですが、こちらも特定のバージョンで[固定することも可能](https://cloud.google.com/spanner/docs/query-optimizer/manage-query-optimizer?hl=ja#db-option)です。

また、大量にデータをロードした直後など、テーブルの内容の変化に統計情報の更新が追いついていない場合には手動で`ANALYZE`コマンドを実行することで統計情報の収集開始を指示することも可能です。

## サイジング

Spannerではノード数の増減によって性能をスケールアウト・インできます。
1ノードは1000 PU（Processing Units/処理ユニット）とも呼ばれ、[1ノードの性能の目安が提供されています](https://cloud.google.com/spanner/docs/performance?hl=ja#typical-workloads)。

リージョナル構成で、1ノードあたり最大で22,500 QPSの読み取り処理能力があります。これはスキーマなどがベストプラクティスに沿った設計の場合での数値であるため、実際のアプリケーションではこれよりも小さい値でCPU使用率などが増設を推奨する基準に到達したりレイテンシが悪化する可能性もあります。実際のワークロードを実行したときの性能がノード数に対して適切であるかの比較対象として参照されると良いです。

ノード数の追加はダウンタイムなく実行できますので、負荷に応じて増減されるのが良いでしょう。

ノードの削除はダウンタイムなしで可能ですが、大幅な削減はアプリケーションに影響を与える可能性があるため、段階的に削減することをオススメします。たとえば、50ノードから10ノードに削減する場合、一度に削減するノード数を全体の10%以下に抑え、削減の間隔を10分以上空けることがオススメです。

## テーブル定義の変更(ALTER TABLE)

Spannerではダウンタイム無しで[テーブル定義が変更できます](https://cloud.google.com/spanner/docs/schema-updates?hl=ja#supported-updates)。つまりスキーマ変更中に対象のテーブルへの読み書きも可能です

一方で、既存の大規模なテーブルへのインデックスの追加や既存データの検証が必要な制約の追加（たとえば既存のカラムに対して、`NOT NULL`を追加するなど）は処理に時間がかかります。そのような場合には、`ALTER TABLE`を実行する前にノードを増やしておくことが有効な場合があります。これはテーブルからインデックスへのデータの書き込み（この処理のことをバックフィルと呼びます）が複数のノードで分散処理ができるようになるため、完了までの時間が短縮できるためです。

また、複数の`ALTER TABLE`を実行する場合には、[まとめて実行すること](https://cloud.google.com/spanner/docs/schema-updates?hl=ja#large-updates)も有効です。これは、Spannerがスキーマをバージョン管理しており、DDL毎にバージョンを更新するのではなくまとめて1回の更新とすることで、バージョン更新の処理を削減することができるためです。

## モニタリング

Spannerでは[さまざまなメトリクス](https://cloud.google.com/monitoring/api/metrics_gcp#gcp-spanner)がCloud Monitoringに記録されます。CPU使用率やリクエスト数や行スキャン数、レイテンシなどです。
これらのメトリクスへの基本的な考え方はRDBと同様ですが、その数値の解釈について特有の考え方などもあります。ここでは差分の解説します。

### CPU使用率

SpannerのCPU使用率は**ノード全体の**平均[CPU 使用率](https://cloud.google.com/spanner/docs/cpu-utilization?hl=ja)がCloud Monitoringメトリクスとして提供されています。
CPU使用率にはユーザー・システムの区分と優先度について高・中・低の3つの区分で集計されます。そのため全体としては6種類あります。通常はユーザーの優先度高がアプリケーションからのクエリや更新処理の実行用として使用されるため、負荷の大半はこちらになるでしょう。

[推奨される最大の CPU 使用率](https://cloud.google.com/spanner/docs/cpu-utilization?hl=ja#recommended-max)は一般的なRDBよりも低いことに注目してださい。たとえば、リージョナル構成では高い優先度の合計が65% 以下を維持することを推奨しています。これを超える場合にはノードを追加することで対応します。

### レイテンシ

Spannerではリクエスト（クエリやDML）の処理にかかったレイテンシをCloud Monitoringのメトリクスとして提供します。レイテンシはウェブコンソール上で50パーセンタイル（p50）と99パーセンタイル（p99）が確認可能です。p50レイテンシはアプリケーションの全般的なレスポンス傾向を判断する指標として重要です。

p99レイテンシについては、とくにテールレイテンシ（tail latency）と呼ぶ場合もあります。テールレイテンシの原因調査については[一本の独立した記事があるほど](https://cloud.google.com/blog/ja/topics/developers-practitioners/how-investigate-high-tail-latency-when-using-cloud-spanner)さまざまな原因が考えられます。多くの場合は明らかな原因があり、回避できます。記事にもある通り、Spannerは分散システムであるという性質から、構成コンポーネントの一部が交換される場合に交換されるコンポーネントで処理してた内容別のコンポーネントを引き継ぐなどを行うことがあり、短時間のテールレイテンシの上昇が完全には避けられないこともあります。テールレイテンシの上昇が継続時間で数分程度の過渡的な事象なのか、継続的に発生している事象であるかは原因と対策を判断する上で重要です。継続的に発生している場合にはクエリの実行効率が悪くなっている、あるいはCPUなどのリソース不足になっている可能性があります。

Cloud Monitoringのメトリクスとして得られるレイテンシはSpanner自身が報告する指標です。一方でアプリケーション側の観点ではリクエストの送信側での測定であるため、2つの指標が一致しない場合、原因はその間にある可能性があります。次節の通りクライアントライブラリはSpannerとの接続を使い回す仕組みがありますが、プールしている接続が不足した場合には新規に接続を行う必要が生じ、これが大きなレイテンシとなる場合があります。

### コネクション数

Spannerではクライアントとの通信にgRPCを使います。gRPCは1つのHTTP/2の接続を複数のチャネルに多重化することができ、Spannerクライアントは1つのチャンネルを1つののセッションとして扱います。Spannerのクライアントライブラリはセッションをプールすることで、新規のセッションを作る必要を減らそうとします。これはRDBでのコネクションプールと似た概念です。

セッションプールは言語毎のクライアント実装によって差はありますが、チューニングできるためテールレイテンシが高くなる場合に変更されると良いでしょう。

### トランザクションへの参加者(Number of Participants)

Spannerではテーブルをスプリットと呼ぶ単位へと分割することでスケーラビリティを向上させています。アプリケーションはトランザクションが幾つのスプリットへアクセスしているかについては意識する必要がありませんが、パフォーマンスの観点ではモニタリングする価値があります。トランザクションで変更を行ったスプリットの数はやや直訳ですが、参加者数（Number of Paticipants）と呼んでいます。プライマリキーで特定の1行を指定し、書き込みをする場合は[単一スプリットへの書き込み](https://cloud.google.com/spanner/docs/whitepapers/life-of-reads-and-writes?hl=ja#single-split_write)となるため話は単純です。しかし、トランザクションでは常複数の行にまたがり処理を行う場合もあります。このような処理では[スプリット間で調停](https://cloud.google.com/spanner/docs/whitepapers/life-of-reads-and-writes?hl=ja#multi-split_write)を行う必要があり、単一スプリットへの書き込みに比べてややコストが高い処理となります。

そのため、Spanner上で実行されているトランザクションがいくつのスプリットに対しての処理だったかを知ることはトランザクションの高いレイテンシを調査する上での手がかりとなります。トランザクションがいくつのスプリットに処理を行おうとしたかについては、Cloud Monitoringでは`transaction_stat/total/participants`という[メトリクス](https://cloud.google.com/monitoring/api/metrics_gcp?hl=ja#gcp-spanner)から得ることができます。時間的推移はこちらをモニタリングすると良いでしょう。

もう1つの確認方法は[トランザクションの統計情報テーブル](https://cloud.google.com/spanner/docs/introspection/transaction-statistics?hl=ja)から得る方法です。この統計情報は時間区切りでSpanner上の[テーブルとして](https://cloud.google.com/spanner/docs/introspection/transaction-statistics?hl=ja#table_schema)記録されており、そのうちで`AVG_PARTICIPANTS`カラムが該当の情報になります。特定のトランザクションが遅いときの原因調査や特定の時間で参加者が大きくなっている場合にトランザクションを特定するときに使用できます。
